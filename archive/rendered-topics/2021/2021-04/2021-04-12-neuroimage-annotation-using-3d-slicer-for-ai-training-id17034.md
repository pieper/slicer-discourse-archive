---
topic_id: 17034
title: "Neuroimage Annotation Using 3D Slicer For Ai Training"
date: 2021-04-12
url: https://discourse.slicer.org/t/17034
---

# Neuroimage annotation using 3D Slicer for AI training 

**Topic ID**: 17034
**Date**: 2021-04-12
**URL**: https://discourse.slicer.org/t/neuroimage-annotation-using-3d-slicer-for-ai-training/17034

---

## Post #1 by @Dhruba (2021-04-12 04:52 UTC)

<p>Operating system: Windows 10<br>
Slicer version: 4.11</p>
<p>I want to train a Neural Network to detect a vessel in CT Perfusion data. For that, I need to annotate some DICOM images using the “3D slicer”. I want the network to detect the vessel by putting a rectangular box on the region. I want to use the ‘MarkupsPlane’ to annotate the images.</p>
<p>Should I save the annotation and DICOM images as “.nrrd” and “.json” files to read in the neural network? I am guessing the “.json” file will work as the ground-truth label for detection.</p>
<p>Also, some of the patients’ data were collected with a scanner that creates 190 slices per scan and some were collected with a scanner that creates 158 slices. So, half of the “.nrrd” has a size of 512x512x190 and the rest has 512x512x158. Can I resize them to a common size (e.g. 128x128x128) before feeding them in the neural net?</p>

---

## Post #2 by @Fernando (2021-04-12 22:29 UTC)

<p>Hi, <a class="mention" href="/u/dhruba">@Dhruba</a>. If you save your images in NRRD, you can use <a href="https://torchio.readthedocs.io/" rel="noopener nofollow ugc">TorchIO</a> to read them and train using patches of that common size.</p>
<p>You can try the transforms using the <a href="https://torchio.readthedocs.io/interfaces/index.html#d-slicer-gui" rel="noopener nofollow ugc">SlicerTorchIO</a> extension.</p>

---

## Post #3 by @Dhruba (2021-04-13 01:34 UTC)

<p>Thanks <a class="mention" href="/u/fernando">@Fernando</a> . But I want to use TensorFlow for implementing the Neural Net. In that case, what should I use? I guess I can use TorchIO only if I implement my DNN using pytorch.<br>
Also, is “.json” file enough for ground truth labeling?</p>

---

## Post #4 by @Fernando (2021-04-13 07:53 UTC)

<p>Why do you want to use TensorFlow? I think it’s possible to mix TorchIO and TensorFlow, but I haven’t tried. I don’t know any currently-maintained framework for medical images with TensorFlow, we are all using PyTorch now (MONAI, TorchIO, nnU-Net, Rising, batchgenerators, medicaltorch…). Maybe you can use pymia. But this is off-topic.</p>
<p>I think JSON is fine, as all you need is 6 numbers representing the bounding box. You could even use a text file, a CSV, a .npy file… It doesn’t really matter, as long as you can read it during training.</p>

---

## Post #5 by @Dhruba (2021-04-13 16:35 UTC)

<p>I have never used PyTorch before. I am comfortable with TensorFlow only. Maybe I will switch over to PyTorch in future.<br>
I have attached a pic of my “.json” file generated by 3D Slicer after using markups Plane. I am not sure which 6 numbers you meant. In my “.json”, there are 3 values in ‘position’, 9 values in ‘orientation’. Also some other numbers for ‘color’ and stuff.</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://us1.discourse-cdn.com/flex002/uploads/slicer/original/3X/6/0/60aeb3d090b3621e961a5f5ec9bcd792ef28b273.png" data-download-href="/uploads/short-url/dNi8odgR1y1mzxKcjjrzdxbtqHV.png?dl=1" title="markups" rel="noopener nofollow ugc"><img src="https://us1.discourse-cdn.com/flex002/uploads/slicer/original/3X/6/0/60aeb3d090b3621e961a5f5ec9bcd792ef28b273.png" alt="markups" data-base62-sha1="dNi8odgR1y1mzxKcjjrzdxbtqHV" width="411" height="500" data-dominant-color="F7F7F7"><div class="meta"><svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">markups</span><span class="informations">777×943 16.5 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg></div></a></div></p>

---

## Post #6 by @Fernando (2021-04-13 16:59 UTC)

<p>I am not familiar with the new Markups system, but that seems to be the contents of a <code>.mrk.json</code> file used to store fiducial points in a markups node. Basically, each 3D point can be represented with 3 coordinates. You mentioned a “rectangular box”, so I assumed you wanted to save an .acsv file representing an ROI node, which is a 3D bounding box that can be represented using 6 numbers, e.g., the center and the axes lengths.</p>

---

## Post #7 by @lassoan (2021-04-14 04:20 UTC)

<p>If you fully define a plane then you will have 3 control points. First control point position is the center, second control points position determines is the x axis direction and x size, third control point determines rotation around x axis and y size of the plane.</p>
<p>You might find the new Markups ROI easier to use. The ROI is axis-aligned by default and specified by center, orientation, and size fields.</p>

---

## Post #8 by @Dhruba (2021-04-14 05:07 UTC)

<p>Thanks <a class="mention" href="/u/lassoan">@lassoan</a> . I am going to use the ROI. I think it is better suited and easier to understand. I have attached the “.acsv” file generated for ROI bounding box. It has 6 values. I think the top 3 points are the center coordinates for x, y , and z. The bottom 3 points represent the length of the box along the x, y, and z-axis. Is this right?</p>
<p><div class="lightbox-wrapper"><a class="lightbox" href="https://us1.discourse-cdn.com/flex002/uploads/slicer/original/3X/f/1/f12558b50a272d359a5c90e8fe45056f8b8488f1.png" data-download-href="/uploads/short-url/yph5goVVErAZJeLPKdkOTVPY3cd.png?dl=1" title="12" rel="noopener nofollow ugc"><img src="https://us1.discourse-cdn.com/flex002/uploads/slicer/optimized/3X/f/1/f12558b50a272d359a5c90e8fe45056f8b8488f1_2_301x500.png" alt="12" data-base62-sha1="yph5goVVErAZJeLPKdkOTVPY3cd" width="301" height="500" srcset="https://us1.discourse-cdn.com/flex002/uploads/slicer/optimized/3X/f/1/f12558b50a272d359a5c90e8fe45056f8b8488f1_2_301x500.png, https://us1.discourse-cdn.com/flex002/uploads/slicer/optimized/3X/f/1/f12558b50a272d359a5c90e8fe45056f8b8488f1_2_451x750.png 1.5x, https://us1.discourse-cdn.com/flex002/uploads/slicer/optimized/3X/f/1/f12558b50a272d359a5c90e8fe45056f8b8488f1_2_602x1000.png 2x" data-dominant-color="F3F3F3"><div class="meta"><svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">12</span><span class="informations">1071×1777 52.1 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg></div></a></div></p>

---
